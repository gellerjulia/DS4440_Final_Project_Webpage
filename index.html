<!doctype html>
<html lang="en">
<head>
<title>Brain Image Classifier</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">


</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Brain Image Classifier</nobr>
 <nobr class="widenobr">For CS 4440</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>Adapting a CNN Brain Tumor Classifier for Wider Use</h2>
<h5>Team Members: Julia Geller & Qi Li</h5>
</div>
</div>
<div class="row">
<div class="col">

<h3>Introduction</h3>

<p>
The goal of our project was to train a Neural Network to classify if an MRI brain image was healthy or contained either a glioma, meningioma, or pituitary tumor. We based our approach off on a publication that reported high accuracy for a model that classified brain images as one of the three aforementioned tumors (Abiwinanda et al., 2020). We wanted to extend the capabilities to also distinguish between tumor and healthy brain scans so the model could fulfill a wider array of physician needs. We tested three model architectures to identify a suitable model for this problem: the model from the paper (Original), a model we created (Adjusted), and a fine-tuned Resnet50 model (Resnet).
</p>

<h3>Paper Review</h3>

<p>
The paper "Brain Tumor Classification Using Convolutional Neural Network" focuses on developing an efficient CNN model for classifying three common types of brain tumors—Glioma, Meningioma, and Pituitary—using T1-weighted contrast-enhanced MRI images. The study simplifies the conventional method of tumor classification by eliminating the need for prior region-based segmentation and feature extraction, thereby streamlining the process. The proposed model, tested on 3,064 images, achieved a training accuracy of 98.51% and a validation accuracy of 84.19%, demonstrating that a simple CNN architecture could yield high accuracy and potentially improve clinical diagnostic processes.
</p>

<p>
Key takeaways include the model's ability to classify tumors with high accuracy using minimal computational resources and without the complexities of tumor segmentation, suggesting significant potential for real-world applications. The findings highlight the power of deep learning in medical image analysis and suggest further exploration into more sophisticated models that could improve accuracy and adaptability to different clinical settings.</p>
</p>

<h3>About the Data</h3>

<p>The data used in this project is a combination of two datasets, referred to as <a href="https://figshare.com/articles/dataset/brain_tumor_dataset/1512427/5"
  >fig share</a> and  <a href="https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri"
  >SARTAJ</a>. 
  Fig share is the data used in the original paper. This data includes T1-weighted MRI brain images for patients with one of three types of tumors: glioma, meningioma, and pituitary. SARTAJ is a Kaggle dataset with MRI brain images of patients with one of the aforementioned tumor types and healthy patients. In our project, we only use the brain images of healthy patients from this dataset. Both datasets had grayscale images. Fig share images were mainly sized 512x512 while SARTAJ images were 233x236, though there was variation in each dataset. Various sizes of the data were tested with different models to create the best performing models. The original and adjusted models both use 64x64 grayscale images while Resnet worked best on 256x254 color (RGB) images.
</p>

<p> 
  We test two splits of data: balanced and imbalance. The balanced split consists of 500 train, 100 validation, and 100 test images for each tumor and 219 train, 50 validation, and 50 test images for the healthy class. Smaller validation and test sets were used for the healthy class as this class consisted of fewer images and we wanted to retain as many images as possible for training. The imbalanced split was created to inflate the number of training samples for the glioma class as it was observed that this class was the class with the largest proportion of incorrect predictions. Imbalanced data had the above-stated splits for all classes, except the glioma class which had 836 train, 50 validation, and 50 test images. Both the original and adjusted models worked best with imbalanced data while Resnet worked best with balanced data, so those datasets were used for their respective models.
</p>


<h3>Models</h3>

<p> We compared the performance and learning abilities of the Adjusted, Original, and Resnet models.</p>

<h4>Adjusted Model</h4>
<p>
  We adjusted the original paper’s architecture in hopes of improving the model for our task. This adjusted model adds depth by incorporating an additional convolutional layer and more filters at each stage, allowing it to capture more detailed features in the brain scan images. The progressive increase in filter depth (16, 32, 64) can help in capturing finer details as the image size reduces, aiming to improve the model's ability to differentiate subtle differences between tumor types and healthy brain tissue. Each maxpool has a kernel size of 2 and stride of 2. All convolutions each have a kernel size of 3. The linear layers had 128 and 4 neurons respectively.
</p>

<h4>Original Model</h4>
<p> This model uses a straightforward architecture with two convolutional layers and max pooling followed by fully connected layers. It's a basic approach designed to capture fundamental patterns in the image data without overfitting, suitable for smaller datasets or less complex image features. Each convolution was 32 filters with a kernel size of 3. Each maxpool kernel size was 2. The linear layer consisted of 64 neurons.</p>

<h4> Resnet Model</h4>
<ul>
  <li>Initial Convolution and Max Pooling:
    <ul>
      <li>The network begins with a 7x7 convolutional layer with 64 filters, stride 2, followed by a 3x3 max pooling layer, also with stride 2. This stage prepares the input for the deeper residual blocks by reducing spatial dimensions and increasing feature depth.</li>
    </ul>
  </li>

  <li>Residual Blocks:
    <ul>
      <li>Stage 1:
        <ul>
          <li>Contains three residual blocks. Each block has three layers: a 1x1 convolution reducing the channels to 64, a 3x3 convolution at 64 filters, and another 1x1 convolution that expands the channels back to 256. The input is added to the output via a skip connection.</li>
        </ul>
      </li>
      <li>Stage 2:
        <ul>
          <li>Similar to stage 1 but with four blocks and the channels increased to 128 in the 3x3 convolution, with the output expanded to 512.</li>
        </ul>
      </li>
      <li>Stage 3:
        <ul>
          <li>Consists of six blocks with 256 filters in the 3x3 convolution layer, expanding the output to 1024.</li>
        </ul>
      </li>
      <li>Stage 4:
        <ul>
          <li>Includes three blocks, boosting the 3x3 convolution filters to 512 with output expanded to 2048.</li>
        </ul>
      </li>
    </ul>
  </li>

  <li>Ending with Global Average Pooling and Fully Connected Layer:
    <ul>
      <li>The network completes with a global average pooling layer that reduces each feature map to a single value, followed by a fully connected layer that outputs the final classification.</li>
    </ul>
  </li>
</ul>


<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Two Tables Side by Side</title>
<style>
.table-container {
  display: flex;
}

.table-container .table {
  margin-right: 20px; /* Adjust spacing between tables */
}

.table {
  font-size: 12px;
}

.table th,
.table td {
  padding: 8px;
  border: 1px solid black;
  text-align: center;
}

.table th {
  background-color: #f2f2f2;
}

/* Increase width of Output Shape column */
.table th:nth-child(2),
.table td:nth-child(2) {
  min-width: 110px; /* Adjust as needed */
  word-wrap: break-word;
}

.table th:nth-child(3),
.table td:nth-child(3) {
  min-width: 70px; /* Adjust as needed */
  word-wrap: break-word;
}
.table h5 {
  text-align: center; /* Center-align the h4 heading */
}
</style>
</head>
<body>

<div class="table-container">
  <div class="table">
    <h5>Adjusted Architecture</h5>
    <table border="1">
   <tr>
     <th>Layer</th>
     <th>Output Shape</th>
     <th>Param #</th>
     <th>Activation</th>
   </tr>
   <tr>
     <td>Input</td>
     <td>(-1, 1, 64, 64)</td>
     <td></td>
     <td></td>
   </tr>
   <tr>
     <td>Conv2d</td>
     <td>(-1, 16, 64, 64)</td>
     <td>160</td>
     <td></td>
   </tr>
   <tr>
     <td>MaxPool2d</td>
     <td>(-1, 16, 32, 32)</td>
     <td></td>
     <td>ReLU</td>
   </tr>
   <tr>
     <td>Conv2d</td>
     <td>(-1, 32, 32, 32)</td>
     <td>4,640</td>
     <td></td>
   </tr>
   <tr>
     <td>MaxPool2d</td>
     <td>(-1, 32, 16, 16)</td>
     <td></td>
     <td>ReLU</td>
   </tr>
   <tr>
     <td>Conv2d</td>
     <td>(-1, 64, 16, 16)</td>
     <td>18,496</td>
     <td></td>
   </tr>
   <tr>
     <td>MaxPool2d</td>
     <td>(-1, 64, 8, 8)</td>
     <td></td>
     <td>ReLU</td>
   </tr>
 
   <tr>
     <td>Linear</td>
     <td>(-1, 128)</td>
     <td>524,416</td>
     <td>ReLU</td>
   </tr>
   <tr>
     <td>Linear</td>
     <td>(-1, 4)</td>
     <td>516</td>
     <td></td>
   </tr>
   <tr>
     <td>Total Parameters:</td>
     <td></td>
     <td>548,228</td>
     <td></td>
   </tr>
    </table>
  </div>
  
  <div class="table">
    <h5>Original Architecture</h5>
    <table border="1">
        <tr>
          <th>Layer</th>
          <th>Output Shape</th>
          <th>Param #</th>
          <th>Activation</th>
        </tr>
        <tr>
          <td>Input</td>
          <td>(-1, 1, 64, 64)</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>Conv2d</td>
          <td>(-1, 32, 62, 32)</td>
          <td>320</td>
          <td>ReLU</td>
        </tr>
        <tr>
          <td>MaxPool2d</td>
          <td>(-1, 32, 31, 31)</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>Conv2d</td>
          <td>(-1, 32, 29, 29)</td>
          <td>9,248</td>
          <td>ReLU</td>
        </tr>
        <tr>
          <td>MaxPool2d</td>
          <td>(-1, 32, 14, 14)</td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>Linear</td>
          <td>(-1, 64)</td>
          <td>401,472</td>
          <td>ReLU</td>
        </tr>
        <tr>
          <td>Linear</td>
          <td>(-1, 4)</td>
          <td>260</td>
          <td></td>
        </tr>
        <tr>
          <td>Softmax</td>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>Total Parameters:</td>
          <td></td>
          <td>411,300</td>
          <td></td>
        </tr>
    </table>
  </div>
    <div class="table">
      <h5>Resnet Architecture</h5>
      <table border="1">
     <tr>
       <th>Layer</th>
       <th>Output Shape</th>
       <th>Param #</th>
       <th>Activation</th>
     </tr>
     <tr>
       <td>Input</td>
       <td>(-1, 1, 256, 254)</td>
       <td></td>
       <td></td>
     </tr>
     <tr>
       <td>Conv2d</td>
       <td>(-1, 64, 128, 127)</td>
       <td>9,408</td>
       <td></td>
     </tr>
     <tr>
       <td>BatchNorm2d</td>
       <td>(-1, 64, 128, 127)</td>
       <td>128</td>
       <td>ReLU</td>
     </tr>
     <tr>
      <td>MaxPool2d</td>
      <td>(-1, 64, 64, 64)</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Conv2d</td>
      <td>(-1, 64, 64, 64)</td>
      <td>4,096</td>
      <td></td>
    </tr>
    <tr>
      <td>BatchNorm2d</td>
      <td>(-1, 64, 64, 64)</td>
      <td>128</td>
      <td>ReLU</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>Bottleneck</td>
      <td>(-1, 2048, 8, 8)</td>
      <td>0</td>
      <td></td>
    </tr>
    <tr>
      <td>AdaptiveAvgPool2d</td>
      <td>(-1, 2048, 1, 1)</td>
      <td>0</td>
      <td></td>
    </tr>
   
     <tr>
       <td>Linear</td>
       <td>(-1, 4)</td>
       <td>8,196</td>
       <td>Softmax</td>
     </tr>
    
     <tr>
       <td>Total Parameters:</td>
       <td></td>
       <td>23,516,228</td>
       <td></td>
     </tr>
      </table>
    </div>

</div>


</body>
</html>


<h4>Training</h4>

<p> 
  All models were trained using Cross Entropy Loss and the Adam optimizer. The adjusted models were trained for 20 epochs with a learning rate of 0.001 while the original paper models were trained for 10 epochs with a learning rate of 0.001 as was specified in the original paper. Resnet was trained for 2 epochs with a learning rate of 0.0001.
</p>



<h3>Results</h3>


<div class="row">
  <div class="column">

    <h5>Adjusted Model</h5>

    <p>Test Loss: 0.596, Test Accuracy: 82%
    </p>

    <p>Figure 1: Confusion Matrix for Adjusted Model</p>

    <img src="graphs/adjusted_matrix.png" width="500" height="500">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 2: Training and Validation Loss and Accuracy plots for Adjusted model</p>
    <img src="graphs/adjusted_train_loss_plot.png" width="550" height=270">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 3: Outputs of convolution 1 of Adjusted model</p>
    <img src="graphs/adjusted_1st_conv.png" width="500" height="500">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 3.1: Outputs of convolution 2 of Adjusted model</p>
    <img src="graphs/adjusted_2nd_conv.png" width="520" height="520">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 3.2: Outputs of convolution 3 of Adjusted model</p>
    <img src="graphs/adjusted_3rd_conv.png" width="550" height="550">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 4: Parameters Gradients of Adjusted model</p>
    <img src="graphs/adjusted_para_grad.png" width="500" height="312.5">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 5: Activations of Adjusted model</p>
    <img src="graphs/adjusted_act.png" width="500" height="312.5">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 6: Activations Parameters of Adjusted model</p>
    <img src="graphs/adjusted_act_grad.png" width="500" height="312.5">
  </div>

  <div class="column">
    
    <h5>Original Model</h5>

    <p>Test Loss: 1.048, Test Accuracy: 68.86%
    </p>

    <p>Figure 1.1: Confusion Matrix for Original model</p>

    <img src="graphs/original_matrix.png" width="500" height="500">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 2.1: Training and Validation Loss and Accuracy plots for Original Model trained on tumor and healthy data</p>
    <img src="graphs/original_train_loss.png" width="550" height="270">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 3.3: Outputs of convolution 1 of Original model</p>
    <img src="graphs/original_1st_conv.png" width="500" height="500">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 3.4: Outputs of convolution 2 of Original model</p>
    <img src="graphs/original_2nd_conv.png" width="520" height="520">

    <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
    <br><br><br><br><br><br><br>

    <p>Figure 4.1: Parameters Gradients of Original model</p>
    <img src="graphs/original_para_grad.png" width="500" height="312.5">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 5.1: Activations of Original model</p>
    <img src="graphs/original_act.png" width="500" height="312.5">

    <br>
    <br>
    <br>
    <br>

    <p>Figure 6.1: Activations Parameters of Original model</p>
    <img src="graphs/original_act_grad.png" width="500" height="312.5">
  </div>
</div>

<h3>Conclusions</h3>
<p>The best model was Resnet with about 94% accuracy on test data. In the medical domain, a type 1 error is when a model incorrectly predicts the presence of disease and a type 2 error is when the model fails to predict an instance of a disease. Type 2 errors must be avoided and it is also highly sought after for type 1 errors to be minimized. The Resnet model had 0 type 1 and 2 errors which is very encouraging. The majority of mistakes were made on class 1 which suggests that training with a higher balance of class 1 samples may improve model performance. While it was not feasible to include all outputs from each convolution as there were too many, interesting convolution outputs were included. Convolutions 1 and 10 appear to pick up on textures and highlight the parietal lobe of the brain. Convolution 2 created more black and white scans of the brain. Convolution 15 was a large layer with low level features of the brain. The last layers including convolutions 39, 49, and 53 tended to have very high level features. 
</p>

<p>
  The next best model was the adjusted model with about 82% accuracy. This model also made most of its mistakes on class 1. The adjusted model also had no type 2 errors and just 6% type 1 errors which is likely acceptable. The validation accuracy was significantly lower than the training accuracy which may suggest the model overfitting to training data, despite different numbers of epochs and learning rates not improving test accuracy. Of the convolutions, the first appeared to pick up on textures, the second grasped lower-level features, and the third highlighted higher-level features. There were issues presented by the gradients that may signify its inability to learn well. The parameter gradients for the first linear layer and activation gradients for all convolutions were centered around 0 which may lead to vanishing gradients and hinder learning ability during training. Despite dropouts being added and not helping model performance, other techniques like batchnorm layers could be tested to improve the model’s learning and overall performance. 
</p>

<p>
  The worst-performing model was the original model from the paper with a test accuracy of about 69%. This model had a lower type 1 error rate than the adjusted model of 2% and made no type 2 errors. Much like the first and third convolutions in the adjusted model, the original model's first convolution picked up on textures and the second convolution picked up on higher-level features. As seen in the adjusted model, the parameter gradients for linear layer 1 and activation gradients for all convolutions were centered around 0 which may lead to vanishing gradients during training. 
</p>

<p>
  Interestingly, none of the models made type 2 errors, signifying they could easily distinguish between healthy and tumor images. The models with more convolutions seemed to be the best, suggesting more convolutions are especially important in this context. It was surprising that Resnet did not achieve higher accuracy given the depth of that model. Since Resnet was the best performing model, in the future we would work to improve the models by upsampling the class 1 data points in the training set or experimenting with different depth Resnets.
</p>


<h3>References</h3>

<p><a name="abiwinanda-2018">[1]</a> <a href="https://link.springer.com/chapter/10.1007/978-981-10-9035-6_33"
  >Nyoman Abiwinanda et. al.
  <em>Brain Tumor Classification Using Convolutional Neural Network.</em></a>
  World Congress on Medical Physics and Biomedical Engineering 2018.
</p>


  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs4440.baulab.info/">About CS 4440</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
